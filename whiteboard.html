<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>White Board</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            text-align: center;
            padding: 20px;
        }

        #notes-list {
            text-align: left;
            max-width: 400px;
            margin: 20px auto;
            display: none; /* Hidden by default until authenticated */
        }

        #authentication-form {
            display: block; /* Visible by default */
        }
    </style>
</head>
<body>

    <div id="authentication-form">
        <h2>Enter Passcode to Access Notes</h2>
        <input type="password" id="passcode" placeholder="Enter Passcode" />
        <button onclick="authenticate()">Login</button>
    </div>

    <div id="notes-list">
        <h1>White Board</h1>
    <!--------------------------------------------------- Your Python code goes here -------------------------------------------------------------------------------------------------->    
    <textarea id="pythonCode" readonly rows="20" cols="80">
    
        import os
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.pipeline import Pipeline
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    # Function to read HTML content from files in a folder
    def read_html_files(folder):
        file_contents = []
        file_labels = []
    
        for filename in os.listdir(folder):
            filepath = os.path.join(folder, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                file_contents.append(file.read())
                # Assuming 'YES' folder is for e-commerce (1) and 'NO' folder is for non-e-commerce (0)
                file_labels.append(1 if folder == 'YES' else 0)
    
        return file_contents, file_labels
    
    # Load data
    e_commerce_contents, e_commerce_labels = read_html_files('html_files/YES')
    non_e_commerce_contents, non_e_commerce_labels = read_html_files('html_files/NO')
    
    # Combine data
    all_contents = e_commerce_contents + non_e_commerce_contents
    all_labels = e_commerce_labels + non_e_commerce_labels
    
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(all_contents, all_labels, test_size=0.2, random_state=42)
    
    # Define models and their respective parameter grids
    models = {
        'RandomForest': (RandomForestClassifier(), {
            'tfidf__max_features': [500, 1000, 2000],
            'classifier__n_estimators': [50, 100, 200],
            'classifier__max_depth': [None, 10, 20],
        }),
        'SVM': (SVC(), {
            'tfidf__max_features': [500, 1000, 2000],
            'classifier__C': [1, 10, 100],
            'classifier__kernel': ['linear', 'rbf'],
        }),
        'LogisticRegression': (LogisticRegression(), {
            'tfidf__max_features': [500, 1000, 2000],
            'classifier__C': [0.1, 1, 10],
        }),
    }
    
    # Variables to keep track of the best model
    best_model_name = None
    best_model = None
    best_accuracy = 0.0
    
    # List to store individual model predictions
    individual_predictions = []
    
    # Iterate over models
    for model_name, (model, param_grid) in models.items():
        # Pipeline with TF-IDF Vectorizer and Classifier
        pipeline = Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('classifier', model)
        ])
    
        # Instantiate GridSearchCV
        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    
        # Fit the model
        grid_search.fit(X_train, y_train)
    
        # Print the best parameters
        print(f"\nBest Parameters for {model_name}:", grid_search.best_params_)
    
        # Evaluate performance using cross-validation
        cv_scores = cross_val_score(grid_search.best_estimator_, all_contents, all_labels, cv=5, scoring='accuracy')
    
        # Print cross-validation scores
        print(f"{model_name} Cross-Validation Scores:", cv_scores)
    
        # Predictions on the test set
        y_pred = grid_search.predict(X_test)
    
        # Evaluate performance on the test set
        accuracy = np.mean(cv_scores)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
    
        # Print evaluation metrics
        print(f"{model_name} Average Cross-Validation Accuracy:", accuracy)
        print(f"{model_name} Precision:", precision)
        print(f"{model_name} Recall:", recall)
        print(f"{model_name} F1 Score:", f1)
    
        # Save only the best model
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model_name = model_name
            best_model = grid_search.best_estimator_
    
        # Save individual model predictions
        individual_predictions.append((model_name, y_pred))
    
    # Ensemble predictions by averaging
    ensemble_predictions = np.mean([y_pred for model_name, y_pred in individual_predictions], axis=0).round().astype(int)
    
    # Evaluate ensemble performance
    ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
    ensemble_precision = precision_score(y_test, ensemble_predictions)
    ensemble_recall = recall_score(y_test, ensemble_predictions)
    ensemble_f1 = f1_score(y_test, ensemble_predictions)
    
    # Print ensemble evaluation metrics
    print("\nEnsemble Model Metrics:")
    print(f"Ensemble Model Accuracy:", ensemble_accuracy)
    print(f"Ensemble Model Precision:", ensemble_precision)
    print(f"Ensemble Model Recall:", ensemble_recall)
    print(f"Ensemble Model F1 Score:", ensemble_f1)
    
    # Save the winning model to a pickle file
    if best_model:
        model_filename = f'{best_model_name}_model.pkl'
        joblib.dump(best_model, model_filename)
        print(f"Trained {best_model_name} model saved to {model_filename}")
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </textarea>
<!--------------------------------------------------- Your Python code goes here -------------------------------------------------------------------------------------------------->

    </div>

    <script>
        const repoOwner = "mauricio5070";
        const repoName = "mauricio5070.github.io";
        const path = "notes.txt";
        const githubToken = "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";
        let authenticated = false;

        function authenticate() {
            const enteredPasscode = document.getElementById("passcode").value;

            if (enteredPasscode === "qq1w2e3") {
                document.getElementById("authentication-form").style.display = "none";
                document.getElementById("notes-list").style.display = "block";
                authenticated = true;
                viewNotes(); // Load existing notes if any
            } else {
                alert("Incorrect passcode. Please try again.");
            }
        }

        function viewNotes() {
            if (!authenticated) {
                alert("Please login to view notes.");
                return;
            }

            fetch(`https://api.github.com/repos/${repoOwner}/${repoName}/contents/${path}`, {
                headers: {
                    "Authorization": `token ${githubToken}`
                }
            })
                .then(response => response.json())
                .then(data => {
                    const notes = atob(data.content).split("\n").filter(note => note.trim() !== "");
                    renderNotes(document.getElementById('notes'), notes);
                })
                .catch(error => {
                    console.error("Error reading notes:", error);
                });
        }

        function addNote() {
            if (!authenticated) {
                alert("Please login to add notes.");
                return;
            }

            const note = prompt('Enter your note:');
            if (note !== null && note.trim() !== '') {
                fetch(`https://api.github.com/repos/${repoOwner}/${repoName}/contents/${path}`, {
                    method: 'PUT',
                    headers: {
                        "Authorization": `token ${githubToken}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        message: "Add new note",
                        content: btoa(`${note.trim()}\n`)
                    })
                })
                    .then(() => {
                        viewNotes(); // Refresh the displayed notes
                        alert('Note added successfully!');
                    })
                    .catch(error => {
                        console.error("Error adding note:", error);
                    });
            }
        }

        function renderNotes(container, notes) {
            container.innerHTML = '';
            notes.forEach((note, index) => {
                const listItem = document.createElement('li');
                listItem.textContent = `${index + 1}: ${note}`;
                container.appendChild(listItem);
            });
        }
    </script>

</body>
</html>

